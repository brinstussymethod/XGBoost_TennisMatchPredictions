{
 "cells": [
  {
   "cell_type": "code",
   "id": "4be0dd6e-c9f6-4789-a885-586ebf3c9165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:54:47.316652Z",
     "start_time": "2025-12-07T21:54:47.219749Z"
    }
   },
   "source": "import sys\nimport os\n\n# Add src folder to path\nsys.path.append('src')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Import modules\nfrom data_loader import TennisDataLoader, load_and_prepare_data\nfrom elo_calculator import calculate_elo_for_dataframe\nfrom feature_engineering import engineer_all_features\nfrom model import TennisPredictionModel\nfrom visualizations import *\n\nprint(\"TENNIS PREDICTION PIPELINE\")\nprint(\"=\"*60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6eccbfc-3513-4954-afc6-7341f97eb31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:54:47.746511Z",
     "start_time": "2025-12-07T21:54:47.319508Z"
    }
   },
   "source": "print(\"\\nSTEP 1: LOADING & CLEANING DATA\")\nprint(\"=\"*60)\n\n# Load data\nloader = TennisDataLoader('data/raw/atp_tennis.csv')\ndf = loader.load_data()\ndf = loader.clean_data()\nloader.get_data_summary()\n\n# Split into train/test\ntrain_df, test_df = loader.split_train_test(test_year=2025)\n\nprint(f\"\\nTraining: {len(train_df):,} matches (2000-2024)\")\nprint(f\"Test: {len(test_df):,} matches (2025)\")\n\n# Save cleaned data\nimport os\nos.makedirs('../data/processed', exist_ok=True)\ntrain_df.to_csv('../data/processed/train_2000_2024.csv', index=False)\ntest_df.to_csv('../data/processed/test_2025.csv', index=False)\nprint(\"\\nData cleaning complete\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c60fd9b9-5614-440b-96df-49e6847a2076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:54:49.641651Z",
     "start_time": "2025-12-07T21:54:47.845800Z"
    }
   },
   "source": "print(\"\\nSTEP 2: CALCULATING ELO RATINGS\")\nprint(\"=\"*60)\nprint(\"This process will take approximately 10-15 minutes.\")\nprint()\n\n# Calculate ELO on ALL data (need continuity from train to test)\nall_data = pd.concat([train_df, test_df]).sort_values('Date').reset_index(drop=True)\nall_with_elo, elo_calc = calculate_elo_for_dataframe(all_data)\n\n# Split back into train/test\ntrain_with_elo = all_with_elo[all_with_elo['Year'] < 2025].copy()\ntest_with_elo = all_with_elo[all_with_elo['Year'] >= 2025].copy()\n\nprint(f\"\\nELO calculation complete\")\nprint(f\"   Training ELO range: {train_with_elo['elo_1'].min():.0f} - {train_with_elo['elo_1'].max():.0f}\")\nprint(f\"   Test ELO range: {test_with_elo['elo_1'].min():.0f} - {test_with_elo['elo_1'].max():.0f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47c2baa9-8837-473a-8a33-e0e1e9408315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:54:54.047386Z",
     "start_time": "2025-12-07T21:54:49.659626Z"
    }
   },
   "source": "print(\"\\nSTEP 3: ENGINEERING FEATURES\")\nprint(\"=\"*60)\nprint(\"This process will take approximately 10-15 minutes.\")\nprint()\n\n# Engineer features for all data\nall_features = engineer_all_features(all_with_elo)\n\n# Split back\ntrain_features = all_features[all_features['Year'] < 2025].copy()\ntest_features = all_features[all_features['Year'] >= 2025].copy()\n\nprint(f\"\\nFeature engineering complete\")\nprint(f\"   Total columns before cleanup: {len(train_features.columns)}\")\n\n# Keep metadata columns for evaluation later\n# These won't be used for training, but we need them for analysis\nmetadata_cols = [\n    'Tournament', 'Date', 'Series', 'Court', 'Surface', 'Round', \n    'Player_1', 'Player_2', 'Winner', 'Score', 'Best of',\n    'Rank_1', 'Rank_2', 'Pts_1', 'Pts_2', 'Odd_1', 'Odd_2',\n    'Year', 'target'\n]\n\n# Check which metadata columns exist\nexisting_metadata = [col for col in metadata_cols if col in train_features.columns]\nprint(f\"   Metadata columns found: {len(existing_metadata)}\")\n\n# Clean up features\nprint(\"\\nCleaning features...\")\n\n# Get all non-numeric columns\nnon_numeric_cols = train_features.select_dtypes(include=['object']).columns.tolist()\n\n# Determine which non-numeric columns to drop (exclude metadata and target)\ncols_to_drop = [col for col in non_numeric_cols \n                if col not in existing_metadata and col != 'target']\n\nif cols_to_drop:\n    print(f\"   Dropping {len(cols_to_drop)} non-numeric non-metadata columns\")\n    train_features = train_features.drop(columns=cols_to_drop)\n    test_features = test_features.drop(columns=cols_to_drop)\n\n# Handle numeric columns only (don't touch metadata strings)\nnumeric_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()\n\n# Replace infinities\ntrain_features[numeric_cols] = train_features[numeric_cols].replace([np.inf, -np.inf], np.nan)\ntest_features[numeric_cols] = test_features[numeric_cols].replace([np.inf, -np.inf], np.nan)\n\n# Fill NaN\ntrain_features[numeric_cols] = train_features[numeric_cols].fillna(0)\ntest_features[numeric_cols] = test_features[numeric_cols].fillna(0)\n\nprint(\"Features cleaned\")\nprint(f\"   Total columns after cleanup: {len(train_features.columns)}\")\nprint(f\"   Numeric feature columns: {len(numeric_cols)}\")\nprint(f\"   Metadata columns preserved: {len(existing_metadata)}\")\n\n# Show which metadata columns we have\nif existing_metadata:\n    print(f\"\\nPreserved for analysis:\")\n    for col in existing_metadata[:10]:  # Show first 10\n        print(f\"   â€¢ {col}\")\n    if len(existing_metadata) > 10:\n        print(f\"   ... and {len(existing_metadata) - 10} more\")\n\n# Save\nos.makedirs('../data/processed', exist_ok=True)\ntrain_features.to_csv('../data/processed/train_features.csv', index=False)\ntest_features.to_csv('../data/processed/test_features.csv', index=False)\nprint(\"\\nFeature data saved\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79310f7c-43aa-4476-9dea-9fcb4dd32d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:55:18.383977Z",
     "start_time": "2025-12-07T21:54:54.069940Z"
    }
   },
   "source": "print(\"\\nSTEP 4: TRAINING MODELS\")\nprint(\"=\"*60)\n\n# Split training data for validation\ntrain_2023 = train_features[train_features['Year'] < 2024].copy()\nval_2024 = train_features[train_features['Year'] == 2024].copy()\n\nprint(f\"   Training: {len(train_2023):,} matches (2000-2023)\")\nprint(f\"   Validation: {len(val_2024):,} matches (2024)\")\n\n# Prepare features for all models\nfrom model import TennisPredictionModel\ntemp_model = TennisPredictionModel()\nX_train, y_train = temp_model.prepare_features(train_2023)\nX_val, y_val = temp_model.prepare_features(val_2024)\n\nprint(f\"   Features: {X_train.shape[1]}\")\n\n# MODEL 1: XGBoost (Baseline)\nprint(\"\\n\" + \"-\"*60)\nprint(\"Model 1: XGBoost (Baseline)\")\nprint(\"-\"*60)\n\nimport xgboost as xgb\nxgb_model = xgb.XGBClassifier(\n    n_estimators=150,\n    learning_rate=0.1,\n    max_depth=6,\n    min_child_weight=1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\nxgb_model.fit(X_train, y_train, verbose=False)\nxgb_val_acc = xgb_model.score(X_val, y_val)\nprint(f\"Validation Accuracy: {xgb_val_acc:.4f} ({xgb_val_acc*100:.2f}%)\")\n\n# MODEL 2: LightGBM (Optimized)\nprint(\"\\n\" + \"-\"*60)\nprint(\"Model 2: LightGBM (Optimized)\")\nprint(\"-\"*60)\n\nimport lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=8,\n    min_child_weight=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    verbose=-1\n)\n\nlgb_model.fit(X_train, y_train)\nlgb_val_acc = lgb_model.score(X_val, y_val)\nprint(f\"Validation Accuracy: {lgb_val_acc:.4f} ({lgb_val_acc*100:.2f}%)\")\n\n# MODEL 3: CatBoost\nprint(\"\\n\" + \"-\"*60)\nprint(\"Model 3: CatBoost\")\nprint(\"-\"*60)\n\nfrom catboost import CatBoostClassifier\ncat_model = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=8,\n    l2_leaf_reg=3,\n    random_state=42,\n    verbose=False\n)\n\ncat_model.fit(X_train, y_train)\ncat_val_acc = cat_model.score(X_val, y_val)\nprint(f\"Validation Accuracy: {cat_val_acc:.4f} ({cat_val_acc*100:.2f}%)\")\n\n# ENSEMBLE: Stacking\nprint(\"\\n\" + \"-\"*60)\nprint(\"Ensemble: Stacking Classifier\")\nprint(\"-\"*60)\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nstacking_model = StackingClassifier(\n    estimators=[\n        ('xgb', xgb_model),\n        ('lgb', lgb_model),\n        ('cat', cat_model)\n    ],\n    final_estimator=LogisticRegression(max_iter=1000),\n    cv=5\n)\n\nstacking_model.fit(X_train, y_train)\nstacking_val_acc = stacking_model.score(X_val, y_val)\nprint(f\"Validation Accuracy: {stacking_val_acc:.4f} ({stacking_val_acc*100:.2f}%)\")\n\n# COMPARISON\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL COMPARISON (Validation 2024)\")\nprint(\"=\"*60)\nprint(f\"XGBoost (Baseline):    {xgb_val_acc:.4f} ({xgb_val_acc*100:.2f}%)\")\nprint(f\"LightGBM:              {lgb_val_acc:.4f} ({lgb_val_acc*100:.2f}%)\")\nprint(f\"CatBoost:              {cat_val_acc:.4f} ({cat_val_acc*100:.2f}%)\")\nprint(f\"Stacking Ensemble:     {stacking_val_acc:.4f} ({stacking_val_acc*100:.2f}%)\")\n\n# Select best model\nbest_models = {\n    'XGBoost': (xgb_model, xgb_val_acc),\n    'LightGBM': (lgb_model, lgb_val_acc),\n    'CatBoost': (cat_model, cat_val_acc),\n    'Stacking': (stacking_model, stacking_val_acc)\n}\nbest_name = max(best_models.items(), key=lambda x: x[1][1])[0]\nmodel, best_val_acc = best_models[best_name]\n\nprint(f\"\\nBest Model Selected: {best_name} ({best_val_acc*100:.2f}%)\")\nprint(\"=\"*60)\n\n# Save all models\nos.makedirs('../models', exist_ok=True)\nimport pickle\nwith open('../models/xgboost_model.pkl', 'wb') as f:\n    pickle.dump(xgb_model, f)\nwith open('../models/lightgbm_model.pkl', 'wb') as f:\n    pickle.dump(lgb_model, f)\nwith open('../models/catboost_model.pkl', 'wb') as f:\n    pickle.dump(cat_model, f)\nwith open('../models/stacking_ensemble.pkl', 'wb') as f:\n    pickle.dump(stacking_model, f)\nwith open('../models/best_model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nprint(\"\\nModels saved to models/ directory\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4187322-8906-4c19-9edc-4d56c4fe3f73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:55:18.518712Z",
     "start_time": "2025-12-07T21:55:18.435030Z"
    }
   },
   "source": "print(\"\\nSTEP 5: MAKING PREDICTIONS ON 2025\")\nprint(\"=\"*60)\n\n# Prepare test features for the best model\nX_test, y_test = temp_model.prepare_features(test_features)\n\n# Make predictions using the best model\npredictions = model.predict(X_test)\nprediction_proba = model.predict_proba(X_test)[:, 1]\n\n# Store predictions in test_features DataFrame\ntest_features['prediction'] = predictions\ntest_features['prediction_proba'] = prediction_proba\n\nprint(f\"Predictions generated using {best_name} model\")\nprint(f\"   Metadata preserved: {', '.join(['Surface', 'Tournament', 'Round']) if all(col in test_features.columns for col in ['Surface', 'Tournament', 'Round']) else 'Some columns missing'}\")\n\n# Save predictions\nos.makedirs('../data/predictions', exist_ok=True)\ntest_features.to_csv('../data/predictions/predictions_2025.csv', index=False)\nprint(\"Predictions saved to data/predictions/\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b35a2a4-1ff4-4a82-bcd4-84010d85770a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:55:18.570213Z",
     "start_time": "2025-12-07T21:55:18.562790Z"
    }
   },
   "source": "print(\"\\nSTEP 6: EVALUATION RESULTS\")\nprint(\"=\"*60)\n\n# Overall accuracy\noverall_accuracy = (test_features['prediction'] == test_features['target']).mean()\ntotal_correct = (test_features['prediction'] == test_features['target']).sum()\ntotal_matches = len(test_features)\n\nprint(f\"\\nOverall 2025 Accuracy: {overall_accuracy:.1%}\")\nprint(f\"   Correct: {total_correct:,} / {total_matches:,} matches\")\n\n# Check what columns we have\nmetadata_cols = ['Surface', 'Tournament', 'Round', 'Series']\navailable_metadata = [col for col in metadata_cols if col in test_features.columns]\n\nif len(available_metadata) > 0:\n    print(f\"\\nAvailable for analysis: {', '.join(available_metadata)}\")\n    \n    # By surface\n    if 'Surface' in test_features.columns:\n        print(f\"\\nAccuracy by Surface:\")\n        for surface in sorted(test_features['Surface'].unique()):\n            surface_data = test_features[test_features['Surface'] == surface]\n            surface_acc = (surface_data['prediction'] == surface_data['target']).mean()\n            print(f\"   {surface}: {surface_acc:.1%} ({len(surface_data)} matches)\")\n    \n    # By tournament type\n    if 'is_grand_slam' in test_features.columns:\n        print(f\"\\nAccuracy by Tournament Type:\")\n        grand_slams = test_features[test_features['is_grand_slam'] == 1]\n        if len(grand_slams) > 0:\n            gs_acc = (grand_slams['prediction'] == grand_slams['target']).mean()\n            print(f\"   Grand Slams: {gs_acc:.1%} ({len(grand_slams)} matches)\")\n        \n        if 'is_masters' in test_features.columns:\n            masters = test_features[test_features['is_masters'] == 1]\n            if len(masters) > 0:\n                masters_acc = (masters['prediction'] == masters['target']).mean()\n                print(f\"   Masters: {masters_acc:.1%} ({len(masters)} matches)\")\nelse:\n    print(f\"\\nMetadata columns were removed during feature engineering\")\n    print(f\"   Can only show overall accuracy\")\n\nprint(\"\\n\" + \"=\"*60)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall 2025 Accuracy: 67.5%\n",
      "   Correct: 1,680 / 2,488 matches\n",
      "\n",
      "Available for analysis: Surface, Tournament, Round, Series\n",
      "\n",
      "Accuracy by Surface:\n",
      "   Clay: 66.3% (733 matches)\n",
      "   Grass: 71.8% (287 matches)\n",
      "   Hard: 67.3% (1468 matches)\n",
      "\n",
      "Accuracy by Tournament Type:\n",
      "   Grand Slams: 74.0% (480 matches)\n",
      "   Masters: 64.4% (748 matches)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e9fa197f-7819-4123-8cec-1529f9d71f7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:55:18.628983Z",
     "start_time": "2025-12-07T21:55:18.621677Z"
    }
   },
   "source": "print(\"\\nWIMBLEDON 2025 ANALYSIS\")\nprint(\"=\"*60)\n\n# Check if we have Tournament column\nif 'Tournament' not in test_features.columns:\n    print(\"\\nTournament column not available\")\n    print(\"   Cannot analyze Wimbledon specifically\")\nelse:\n    wimbledon = test_features[test_features['Tournament'] == 'Wimbledon'].copy()\n    \n    if len(wimbledon) > 0:\n        wimb_accuracy = (wimbledon['prediction'] == wimbledon['target']).mean()\n        wimb_correct = (wimbledon['prediction'] == wimbledon['target']).sum()\n        \n        print(f\"\\nOverall Accuracy: {wimb_accuracy:.1%} ({wimb_correct}/{len(wimbledon)} matches)\")\n        \n        # By round (if available)\n        if 'Round' in wimbledon.columns:\n            print(f\"\\nAccuracy by Round:\")\n            for round_name in ['1st Round', '2nd Round', '3rd Round', '4th Round', \n                               'Quarterfinals', 'Semifinals', 'The Final']:\n                round_data = wimbledon[wimbledon['Round'] == round_name]\n                if len(round_data) > 0:\n                    round_acc = (round_data['prediction'] == round_data['target']).mean()\n                    print(f\"   {round_name}: {round_acc:.1%} ({len(round_data)} matches)\")\n        \n        # Final match (if available)\n        if 'Round' in wimbledon.columns:\n            final = wimbledon[wimbledon['Round'] == 'The Final']\n            if len(final) > 0:\n                print(f\"\\nWIMBLEDON 2025 FINAL:\")\n                final_row = final.iloc[0]\n                if 'Player_1' in final_row and 'Player_2' in final_row:\n                    print(f\"   {final_row['Player_1']} vs {final_row['Player_2']}\")\n                    print(f\"   Winner: {final_row['Winner']}\")\n                    predicted_winner = final_row['Player_1'] if final_row['prediction'] == 1 else final_row['Player_2']\n                    print(f\"   Predicted: {predicted_winner}\")\n                    correct_text = \"YES\" if final_row['prediction'] == final_row['target'] else \"NO\"\n                    print(f\"   Correct: {correct_text}\")\n                    print(f\"   Confidence: {final_row['prediction_proba']:.1%}\")\n    else:\n        print(\"\\nNo Wimbledon 2025 data found\")\n\nprint(\"\\n\" + \"=\"*60)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WIMBLEDON 2025 ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 72.1% (88/122 matches)\n",
      "\n",
      "Accuracy by Round:\n",
      "   1st Round: 68.9% (61 matches)\n",
      "   2nd Round: 65.6% (32 matches)\n",
      "   3rd Round: 81.2% (16 matches)\n",
      "   4th Round: 100.0% (6 matches)\n",
      "   Quarterfinals: 100.0% (4 matches)\n",
      "   Semifinals: 100.0% (2 matches)\n",
      "   The Final: 0.0% (1 matches)\n",
      "\n",
      "WIMBLEDON 2025 FINAL:\n",
      "   Alcaraz C. vs Sinner J.\n",
      "   Winner: Sinner J.\n",
      "   Predicted: Alcaraz C.\n",
      "   Correct: NO\n",
      "   Confidence: 57.6%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ff225deb-116b-4f78-986d-2cbc0a96a5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T21:55:18.686224Z",
     "start_time": "2025-12-07T21:55:18.683137Z"
    }
   },
   "source": "print(\"\\nPIPELINE COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\nFinal Results ({best_name}):\")\nprint(f\"   Overall 2025 Accuracy: {overall_accuracy:.1%}\")\nprint(f\"   Total Predictions: {total_matches:,}\")\nprint(f\"   Correct Predictions: {total_correct:,}\")\nprint(f\"\\nModel Performance:\")\nprint(f\"   Validation (2024): {best_val_acc:.1%}\")\nprint(f\"   Test (2025): {overall_accuracy:.1%}\")\nprint(f\"\\nComparison:\")\nprint(f\"   Random Guessing: 50.0%\")\nprint(f\"   Model Performance: {overall_accuracy:.1%}\")\nprint(f\"   Typical Betting Odds: ~70-72%\")\nprint(\"\\nResults saved to data/predictions/\")\nprint(\"=\"*60)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}